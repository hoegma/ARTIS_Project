\section{Methodology}
\label{sec:old-methods}

\subsection{Experimental Setup}

A fake face detection model is developed using federated learning (FL) to distinguish between real and fake faces.
First, a centrally trained model without FL is implemented and used as a baseline for performance comparison.
Second, a decentralized fake face detection model is trained using FL in a simulated multi-client environment.

\subsubsection{Dataset}

The well-known 140k real-fake faces dataset was used, which consists of 70,000 real faces and 70,000 fake faces with a image size of 256px~\cite{tunguz2020_140kreal_fake_faces}.
The fake faces were generated using StyleGAN, a generative adversarial network developed by NVIDIA that is capable of producing highly photorealistic synthetic facial images.

\subsubsection{Model}\label{subsubsec:model}

The base model chosen is EfficientNet-B0, a convolutional neural network (CNN), which was also used by Khudeyer et al.~\cite{khudeyer2023fake}.
The authors trained a fake face detection model using transfer learning.
For this purpose, the model was initialized with the pretrained weights of EfficientNetB0 on the ImageNet dataset.
A lightweight head was attached to the pre-trained base model, consisting of global average pooling, a 256-dimensional fully connected layer with ReLU activation, batch normalization, and dropout, followed by a 2 dimensional softmax output layer.
The output is a 2-dimensional vector with probabilities indicating whether the input image is a fake or real face.
The model was optimized with binary cross-entropy loss and the Adam optimizer.
All input images for training were resized to 244px.

\subsection{Centralized Fake Face Detection Model}

The authors of~\cite{khudeyer2023fake} developed a method for fake face detection using CNN, which achieves an accuracy of 99.06\%.
This approach is used as a benchmark for comparing central fake face detection with fake face detection using FL.
This work was reimplemented for verification purposes in order to ensure a meaningful comparison.
\newline
The dataset was divided into 100.000 training images, 20.000 test images, and 20.000 validation images.
EfficientNetB0 with the hyperparameters of section~\ref{subsubsec:model} is used.
Training was performed with a batch size of 32 over 30 epochs with early stopping to reduce training time.
\newline
The paper presented a learning rate scheduler that adjusts the learning rate during training based on the epoch, as shown in Table~\ref{tab:learning_rate_scheduler}.
\begin{table}[t]
    \centering
    \begin{tabular}{l c}
    \hline
    \textbf{Epoch} & \textbf{Learning rate} \\
    \hline
    $epoch \leq 2$ & 0.01 \\
    $2 < epoch \leq 15$ & 0.001 \\
    $epoch > 15$ & 0.0001 \\
    \hline
    \end{tabular}
    \caption{Adjustment of the learning rate during training.}
    \label{tab:learning_rate_scheduler}
\end{table}
The learning rate scheduler ensures that significant weight adjustments are made early in training.
Furthermore, in later iterations, a strong adjustment is prevented by the decreasing learning rate.
This leads to faster convergence in early epochs, while weight optimizations can be performed in later itterations.
\newline
Due to time constraints, the model was only trained once, as training was very computationally intensive due to the large amount of data.

\subsection{Decentralized Fake Face Detection using FL}

In this section a decentralized trained fake face detection model is developed.
The training process will be explained below.
In addition, we will discuss how privacy attacks can be prevented during training.

\subsubsection{Research Scenario}

The following scenario is fictional.
\newline
Several research organizations want to work together to train a fake face detection model.
Each individual organization has images of fake faces, but also images of faces that belong to their customers.
An ML model should be trained together that can distinguish between real and fake images.
To do this, a large data set containing all images would have to be created in order to train the model.
However, all organizations are interested in protecting the privacy of their customers and therefore do not want to share the real images.
The solution is to train the model using FL.
This fake face detection model should be trained with all data from all organizations and should be available to everyone
without the need to share data between organizations.

\subsubsection{Thread Model}

All participants in the fake face detection training process are trusted.
This means that all model performance attacks that seek to undermine the convergence of the global model can be excluded.
After training, participants send their model weights to a trusted third-party server.
This ensures the secure aggregation of weights.
Weights sent by participants to the trusted third party could be captured during transmission.
Capturing the weights of individual organizations represents an attack vector for privacy attacks.
This attack vector should be reduced by the proposed encryption.

\subsubsection{FL Simulation}

A simulation environment is developed to simulate a global model with any number of participants.
The EfficientNetB0 with the hyperparameters of section~\ref{subsubsec:model} is used as the base model.
A batch size of 128 is selected.
The training data set was divided among five participants, with each receiving approximately 20,000 images, comprising 10,000 real faces and 10,000 fake faces.
On the server, the global model is initialized with the weights from EfficientNetB0, which was pre-trained on the ImageNet dataset.
The global model is trained in 10 epochs.
\newline
Each epoch consists of the following steps.
\newline
First, the weights of the global model are distributed among the 5 participants.
Each participant initializes their local model with the global weights.
The participants train their local models in 3 epochs.
The weights of the local models are encrypted to prevent privacy attacks by intercepting the weights.
The server receives the weights and decrypts them.
The global model parameters are updated using weighted federated averaging (FedAvg), where each client's contribution is proportional to the number of local samples.

\subsubsection{FL attack mitigations}

If the local updates of the participants are captured during transmission to the server, privacy attacks on the participants' data can be carried out.
To counteract this, the weight updates must be encrypted.
The weights are serialized in bytes and encrypted with AES-128.
Each participant has a secret key that the server knows as a trusted third party.
When a server receives the encrypted data, it can decrypt it for further processing.

\subsection{Explainability using SHAP}

In order to analyze the centrally trained model and the decentralized trained model, the decisions made by the models should be compared for individual inputs.
The aim is to clarify whether the models have developed differently as a result of the different training processes and whether they focus on different details when making decisions.
The SHAP method is used for this purpose.





