\section{Background}
\label{sec:Background}

\subsection{Digital Forensics and the use of Machine Learning}

A perpetrator always leaves traces of evidence of their involvement at the crime scene, as described by Dr. Edmond Locard in his exchange principles, which are used in forensic science ~\cite{bode2019every}.
\newline
The landscape of forensic science has changed due to the rise of electronic devices, which play an increasingly important role in our daily lives and are often connected to the internet and accessible from anywhere.
The field has expanded since the early 2000s with digital forensics (DF), which specializes more in the growing number of cybercrimes.
However, even crimes that are not classified as cybercrime are becoming increasingly digital in most modern crime scenes.
According to the EU, digital evidence is involved in 85\% of criminal investigations.
This evidence consists of data generated in our daily lives through the use of digital devices, leaving behind a digital footprint.
The footprint consists of data generated by wearable devices, emails, cloud service providers, online payments, and other sources~\cite{casino2022research}.
\newline
The field of digital forensics can be divided into seven identifiable sub-areas, namely blockchain, networks, mobile, cloud, IoT, file systems \& data storage, and multimedia.
This project is limited to the sub-area of multimedia, which specializes in image forger~\cite{casino2022research}.
\newline
A major challenge in this field is dealing with large, complex data sets and classifying them.
This problem can be addressed by machine learning (ML), whose techniques have expanded and improved in recent years.
ML techniques search through the data and look for anomalies and patterns in the investigation process.
The largest area for ML in DF is image forensics, accounting for 62.7\%.
In the field of image forensics, convolutional neural networks (CNNs) are typically used to recognize such complex patterns in the data, 
which is why this approach was pursued in the project.~\cite{nayerifard2023machine}

\subsection{CNNs for face fake detection}

\subsubsection{Fundamentals}

In recent years, there has been intensive research into CNNs in the areas of image classification, facial recognition, and facial expressions, resulting in significant improvements.
However, with the emergence of increasingly sophisticated deepfakes, it is becoming more and more difficult to distinguish between real and fake faces~\cite{khudeyer2023fake}.
During the training process, CNNs learn complex patterns and can provide information about whether an input image is fake or real.
Among CNN architectures, EfficientNet is one of the most modern models, as it is faster, has fewer parameters and has more capabilities for extracting features than other CNN models.
These parameters are also called weights and can be learned by the model during the training process.
During the training process, the model is shown many input images of positive (real faces) and negative (fake faces)~\cite{wang2019development}.
The model independently learns complex patterns in the form of weights over several iterations (epochs) in order to distinguish positive from negative examples in the case of a binary classifier, or to assign probabilities to classes in the case of a multiple classifier system.~\cite{lorena2008review}\cite{hadjadji2014multiple}.
What is special about CNNs are the convolutional layers, which reduce the resolution of the images and extract the spatial local features through weighted convolutions.
Such local features can be low-level features such as edges, end points and corners in the first convolutional layers and complex high-level features in the last layers.
In the final layers, the high-level features (3D vector) are combined into a fully connected layer (1D) to make a classification decision.
In this project, the output consists of two output nodes with probabilities indicating whether the input image is a fake or real face~\cite{wang2019development}.

\subsubsection{Transfer Learning}

To avoid training a model from scratch for a task, a pre-trained model can be reused and retrained for a new task.
This process is called transfer learning.
To do this, the fully connected layers are replaced with the number of nodes required for the new task.
In the case of fake face detection classification, two nodes are required for the probabilities of the image being a real or fake face.
Furthermore, the weights in the first layers (close to the input) are frozen so that the parameters are not changed during the training process.
These weights represent what has been learned from the original model and should remain as consistent as possible.
The weights in the back layers can then be adjusted to the new task during training by fine-tuning~\cite{khudeyer2023fake}.

\subsubsection{Model Explainability using SHAP}

TBD

\subsection{Federated Learning}

\subsubsection{Fundamentals}
In the age of big data, data privacy and security are becoming increasingly important and are receiving more attention. 
The in 2018 published GDPR aims to protect individuals' data, users of services should be able to have their data deleted at any time and data may not be used for training ML models without consent.
This leads to a problem in training classical ML models, as large amounts of data form the basis for them.
However, the data volumes are now not stored centrally in a single data set but scattered across data islands.
Federated learning addresses this problem by training ML models with data from data islands without moving the data from the islands, thereby preserving privacy.~\cite{zhang2021survey}
\newline \newline
To perform FL, three main steps must be carried out, as described in~\cite{mohamed2024harnessing}.
\newline
1 - Model selection: A global model must be selected, which is initialized with pre-trained weights to speed up the learning process. The global model is located on a central server. 
\newline
2 - Model training: The parameters of the global model are distributed to all clients. 
Each of the clients owns a portion of the data for the training process. 
The clients initialize their local model with the parameters of the global model and train it for several epochs.
\newline
3 - Parameter aggregation: All clients send their local parameters to the server. The server updates the global model with an algorithm that incorporates the local updates from the clients into the global model.
\newline
The last two steps are repeated until a specified number of epochs or a desired accuracy has been achieved.

\subsection{The Flower Federated Learning Framework}
\label{subsec:flower-framework}

Flower is an open-source framework designed for FL applications. It provides a flexible and extensible architecture that enables machine learning across decentralized data while maintaing data privacy on client devices.

\subsubsection*{Key Advantages}

\begin{enumerate}
  
    \item \textbf{Framework Agnosticism}: Flower supports many ML frameworks such as PyTorch, TensorFlow, JAX, and scikit-learn. This allowed us freedom in development without compatibility issues.
    
    \item \textbf{Low Learning Curve and Ease of Use}: Flower is specifically designed with developer experience in mind. Its clean, minimal API allows for quick prototypingâ€”a working FL system can be implemented with just a few lines of code. Compared to alternatives like TensorFlow Federated (which requires learning specialized abstractions) or PySyft (with its complex privacy-preserving cryptography integration), Flower offers a more intuitive approach that reduces the barrier to entry for both researchers and practitioners.
    
    \item \textbf{Scalability}: The framework supports deployments ranging from small-scale simulations to production systems with hundreds of clients. This scalability enables both algorithm development in simulated environments and real-world deployment across distributed devices.
    
    \item \textbf{Production Readiness}: Beyond research use cases, Flower includes features necessary for production deployment, including fault tolerance, client management, and monitoring capabilities. This makes it suitable for both experimental validation and real-world applications.
\end{enumerate}


\subsubsection{Attack Vectors in FL}

Attack vectors on FL are divided into two large groups, which are described in~\cite{neto2023survey}.
\newline
On the one hand, there are model performance attacks, which focus on damaging the training process in various ways.
These include data poisoning attacks, in which the training data is modified by replacing images or labels before the training.
Model poisoning attacks, in which the gradients of the locally trained model are altered.
Free-riding attacks, in which the model is only used without adding value to the global model.
\newline
On the other hand, there are privacy attacks that attempt to draw conclusions about training data from the local parameters of clients or the global parameters of the global model.
These include model inversion and gradient inference attacks, as well as GAN reconstruction attacks, 
in which private training data is to be reconstructed. 




