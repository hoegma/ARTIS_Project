{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":939937,"sourceType":"datasetVersion","datasetId":501529}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Federated Learning Implementation for Fake Image Detection\n# Based on: \"Fake Image Detection Using Deep Learning\"\n\nimport os\nimport numpy as np\nimport pickle\nimport datetime\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras import backend as K\nimport gc\n\n# -------------------------------------------------------------\n# CONFIG\n# -------------------------------------------------------------\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 128\nLOCAL_EPOCHS = 3          # Epochs per client per round\nFEDERATED_ROUNDS = 10     # Number of federated rounds\nNUM_CLIENTS = 5           # Number of federated clients\n\nBASE_PATH = \"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake\"\nTRAIN_PATH = f\"{BASE_PATH}/train\"\nVAL_PATH = f\"{BASE_PATH}/valid\"\nTEST_PATH = f\"{BASE_PATH}/test\"\n\nAUTOTUNE = tf.data.AUTOTUNE\n\n# -------------------------------------------------------------\n# GPU Setup\n# -------------------------------------------------------------\ndef setup_gpu():\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            print(f\"✓ GPU enabled: {len(gpus)} GPU(s)\")\n        except RuntimeError as e:\n            print(f\"GPU error: {e}\")\n    else:\n        print(\"⚠ No GPU - using CPU\")\n\n# -------------------------------------------------------------\n# Create Model (same architecture as paper)\n# -------------------------------------------------------------\ndef create_model():\n    \"\"\"Create EfficientNetB0 model with custom top layers\"\"\"\n    base_model = EfficientNetB0(\n        weights=\"imagenet\",\n        include_top=False,\n        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n    )\n    base_model.trainable = False\n\n    model = models.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(256, activation=\"relu\"),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(2, activation=\"softmax\", dtype='float32')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n    )\n\n    return model\n\n# -------------------------------------------------------------\n# Load Full Dataset\n# -------------------------------------------------------------\ndef load_full_dataset():\n    \"\"\"Load the complete training, validation, and test datasets\"\"\"\n    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        TRAIN_PATH,\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        label_mode=\"categorical\",\n        shuffle=True,\n        seed=42\n    )\n\n    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        VAL_PATH,\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        label_mode=\"categorical\"\n    )\n\n    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        TEST_PATH,\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        label_mode=\"categorical\"\n    )\n\n    # Preprocessing\n    train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n    val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n    test_ds = test_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n\n    # Optimize\n    train_ds = train_ds.prefetch(AUTOTUNE)\n    val_ds = val_ds.prefetch(AUTOTUNE)\n    test_ds = test_ds.prefetch(AUTOTUNE)\n\n    return train_ds, val_ds, test_ds\n\n# -------------------------------------------------------------\n# Partition Data for Federated Clients (shard-based, no full materialization)\n# -------------------------------------------------------------\ndef partition_data_for_clients(train_ds, num_clients):\n    \"\"\"\n    Split training data among clients using tf.data.shard (no full RAM load)\n    \"\"\"\n    # Estimate total samples in a streamed way (one pass)\n    total_samples = 0\n    for batch_x, _ in train_ds:\n        total_samples += batch_x.shape[0]\n    print(f\"Total training samples: {total_samples}\")\n\n    client_datasets = []\n    client_sample_counts = []\n\n    # Create a sharded view for each client\n    for client_id in range(num_clients):\n        client_ds = train_ds.shard(num_shards=num_clients, index=client_id)\n        # Approximate per-client sample count\n        client_samples = total_samples // num_clients\n        client_datasets.append(client_ds.prefetch(AUTOTUNE))\n        client_sample_counts.append(client_samples)\n        print(f\"  Client {client_id + 1}: ~{client_samples} samples\")\n\n    return client_datasets, client_sample_counts\n\n# -------------------------------------------------------------\n# Federated Learning: Weight Aggregation\n# -------------------------------------------------------------\ndef aggregate_weights(client_weights_list, client_sample_counts):\n    \"\"\"\n    Federated Averaging (FedAvg) - WEIGHTED average based on client data size\n    \"\"\"\n    total_samples = sum(client_sample_counts)\n\n    # Initialize with zeros - use float32 for ALL weight arrays\n    first_weights = client_weights_list[0]\n    avg_weights = [np.zeros_like(w, dtype=np.float32) for w in first_weights]\n\n    # Weighted sum of client weights\n    for client_weights, num_samples in zip(client_weights_list, client_sample_counts):\n        client_weight = num_samples / total_samples\n        \n        for i, w in enumerate(client_weights):\n            # Ensure everything is float32\n            avg_weights[i] += np.float32(client_weight) * w.astype(np.float32)\n\n    return avg_weights\n# -------------------------------------------------------------\n# Client Training Function (with memory cleanup)\n# -------------------------------------------------------------\ndef train_client(client_id, client_dataset, global_weights, local_epochs):\n    \"\"\"\n    Train a single client on their local data\n    \"\"\"\n    print(f\"  Training Client {client_id}...\")\n\n    # Create fresh model with global weights\n    K.clear_session()  # clear any previous model graph\n    model = create_model()\n    model.set_weights(global_weights)\n\n    # Train on local data\n    history = model.fit(\n        client_dataset,\n        epochs=local_epochs,\n        verbose=0\n    )\n\n    # Get updated weights (this is what gets sent to server)\n    updated_weights = model.get_weights()\n\n    # Get training metrics\n    final_loss = history.history['loss'][-1]\n    final_acc = history.history['accuracy'][-1]\n\n    print(f\"    Client {client_id} - Loss: {final_loss:.4f}, Acc: {final_acc:.4f}\")\n\n    # Explicit cleanup to reduce memory usage on Kaggle\n    del history\n    gc.collect()\n    K.clear_session()\n\n    return updated_weights, final_loss, final_acc\n\n# -------------------------------------------------------------\n# Federated Learning Main Loop\n# -------------------------------------------------------------\ndef federated_learning(client_datasets, client_sample_counts, val_ds, test_ds):\n    \"\"\"\n    Main federated learning loop\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"FEDERATED LEARNING - FAKE IMAGE DETECTION\")\n    print(\"=\"*60)\n    print(f\"Clients: {NUM_CLIENTS}\")\n    print(f\"Federated Rounds: {FEDERATED_ROUNDS}\")\n    print(f\"Local Epochs per Round: {LOCAL_EPOCHS}\")\n    print(f\"\\nClient Data Distribution:\")\n    total = sum(client_sample_counts)\n    for i, count in enumerate(client_sample_counts):\n        percentage = (count / total) * 100\n        print(f\"  Client {i+1}: {count} samples (~{percentage:.1f}%)\")\n    print(\"=\"*60 + \"\\n\")\n\n    # Initialize global model\n    global_model = create_model()\n    global_weights = global_model.get_weights()\n\n    # Track metrics\n    history = {\n        'round': [],\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': [],\n        'val_auc': []\n    }\n\n    # Federated training rounds\n    for round_num in range(1, FEDERATED_ROUNDS + 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"FEDERATED ROUND {round_num}/{FEDERATED_ROUNDS}\")\n        print(f\"{'='*60}\")\n\n        # Store weights from all clients\n        client_weights_list = []\n        round_losses = []\n        round_accs = []\n\n        # Train each client\n        for client_id in range(1, NUM_CLIENTS + 1):\n            client_weights, loss, acc = train_client(\n                client_id=client_id,\n                client_dataset=client_datasets[client_id - 1],\n                global_weights=global_weights,\n                local_epochs=LOCAL_EPOCHS\n            )\n            client_weights_list.append(client_weights)\n            round_losses.append(loss)\n            round_accs.append(acc)\n\n        # Aggregate weights (WEIGHTED Federated Averaging)\n        print(f\"\\n  Aggregating weights (weighted by dataset size)...\")\n        global_weights = aggregate_weights(client_weights_list, client_sample_counts)\n\n        # Update global model\n        global_model.set_weights(global_weights)\n\n        # Evaluate on validation set\n        print(f\"  Evaluating global model on validation set...\")\n        val_results = global_model.evaluate(val_ds, verbose=0)\n        val_loss, val_acc, val_auc = val_results\n\n        # Average client metrics\n        avg_train_loss = np.mean(round_losses)\n        avg_train_acc = np.mean(round_accs)\n\n        # Store metrics\n        history['round'].append(round_num)\n        history['train_loss'].append(avg_train_loss)\n        history['train_acc'].append(avg_train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['val_auc'].append(val_auc)\n\n        print(f\"\\n  Round {round_num} Summary:\")\n        print(f\"    Avg Client Train Loss: {avg_train_loss:.4f}\")\n        print(f\"    Avg Client Train Acc:  {avg_train_acc:.4f}\")\n        print(f\"    Global Val Loss:       {val_loss:.4f}\")\n        print(f\"    Global Val Acc:        {val_acc:.4f}\")\n        print(f\"    Global Val AUC:        {val_auc:.4f}\")\n\n        # Save checkpoint\n        if round_num % 2 == 0:  # Save every 2 rounds\n            global_model.save(f\"federated_model_round_{round_num}.keras\")\n            print(f\"    ✓ Checkpoint saved\")\n\n    return global_model, history\n\n# -------------------------------------------------------------\n# Main Function\n# -------------------------------------------------------------\ndef main():\n    setup_gpu()\n\n    print(\"\\n=== Loading Dataset ===\")\n    train_ds, val_ds, test_ds = load_full_dataset()\n\n    print(\"\\n=== Partitioning Data for Federated Clients ===\")\n    client_datasets, client_sample_counts = partition_data_for_clients(train_ds, NUM_CLIENTS)\n\n    print(\"\\n=== Starting Federated Learning ===\")\n    global_model, history = federated_learning(client_datasets, client_sample_counts, val_ds, test_ds)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL EVALUATION ON TEST SET\")\n    print(\"=\"*60)\n    test_results = global_model.evaluate(test_ds, verbose=1)\n    test_loss, test_acc, test_auc = test_results\n\n    print(f\"\\nFinal Test Results:\")\n    print(f\"  Test Loss:     {test_loss:.4f}\")\n    print(f\"  Test Accuracy: {test_acc*100:.2f}%\")\n    print(f\"  Test AUC:      {test_auc:.4f}\")\n\n    # Save final model and history\n    global_model.save(\"federated_final_model.h5\")\n    with open(\"federated_history.pkl\", \"wb\") as f:\n        pickle.dump(history, f)\n\n    print(\"\\n✓ Training Complete!\")\n    print(\"✓ Model saved as 'federated_final_model.h5'\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:38:50.055725Z","iopub.execute_input":"2025-12-28T14:38:50.056449Z","iopub.status.idle":"2025-12-28T18:24:19.574220Z","shell.execute_reply.started":"2025-12-28T14:38:50.056415Z","shell.execute_reply":"2025-12-28T18:24:19.572627Z"}},"outputs":[{"name":"stdout","text":"✓ GPU enabled: 1 GPU(s)\n\n=== Loading Dataset ===\nFound 100000 files belonging to 2 classes.\nFound 20000 files belonging to 2 classes.\nFound 20000 files belonging to 2 classes.\n\n=== Partitioning Data for Federated Clients ===\nTotal training samples: 100000\n  Client 1: ~20000 samples\n  Client 2: ~20000 samples\n  Client 3: ~20000 samples\n  Client 4: ~20000 samples\n  Client 5: ~20000 samples\n\n=== Starting Federated Learning ===\n\n============================================================\nFEDERATED LEARNING - FAKE IMAGE DETECTION\n============================================================\nClients: 5\nFederated Rounds: 10\nLocal Epochs per Round: 3\n\nClient Data Distribution:\n  Client 1: 20000 samples (~20.0%)\n  Client 2: 20000 samples (~20.0%)\n  Client 3: 20000 samples (~20.0%)\n  Client 4: 20000 samples (~20.0%)\n  Client 5: 20000 samples (~20.0%)\n============================================================\n\n\n============================================================\nFEDERATED ROUND 1/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.3790, Acc: 0.8286\n  Training Client 2...\n    Client 2 - Loss: 0.3789, Acc: 0.8306\n  Training Client 3...\n    Client 3 - Loss: 0.3769, Acc: 0.8303\n  Training Client 4...\n    Client 4 - Loss: 0.3796, Acc: 0.8308\n  Training Client 5...\n    Client 5 - Loss: 0.3788, Acc: 0.8291\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 1 Summary:\n    Avg Client Train Loss: 0.3787\n    Avg Client Train Acc:  0.8299\n    Global Val Loss:       0.3895\n    Global Val Acc:        0.8175\n    Global Val AUC:        0.9048\n\n============================================================\nFEDERATED ROUND 2/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.3410, Acc: 0.8516\n  Training Client 2...\n    Client 2 - Loss: 0.3441, Acc: 0.8525\n  Training Client 3...\n    Client 3 - Loss: 0.3462, Acc: 0.8465\n  Training Client 4...\n    Client 4 - Loss: 0.3388, Acc: 0.8531\n  Training Client 5...\n    Client 5 - Loss: 0.3414, Acc: 0.8485\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 2 Summary:\n    Avg Client Train Loss: 0.3423\n    Avg Client Train Acc:  0.8504\n    Global Val Loss:       0.3357\n    Global Val Acc:        0.8512\n    Global Val AUC:        0.9315\n    ✓ Checkpoint saved\n\n============================================================\nFEDERATED ROUND 3/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.3101, Acc: 0.8665\n  Training Client 2...\n    Client 2 - Loss: 0.3098, Acc: 0.8651\n  Training Client 3...\n    Client 3 - Loss: 0.3200, Acc: 0.8609\n  Training Client 4...\n    Client 4 - Loss: 0.3179, Acc: 0.8612\n  Training Client 5...\n    Client 5 - Loss: 0.3166, Acc: 0.8624\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 3 Summary:\n    Avg Client Train Loss: 0.3149\n    Avg Client Train Acc:  0.8632\n    Global Val Loss:       0.3232\n    Global Val Acc:        0.8577\n    Global Val AUC:        0.9367\n\n============================================================\nFEDERATED ROUND 4/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.2934, Acc: 0.8763\n  Training Client 2...\n    Client 2 - Loss: 0.2934, Acc: 0.8741\n  Training Client 3...\n    Client 3 - Loss: 0.2983, Acc: 0.8723\n  Training Client 4...\n    Client 4 - Loss: 0.2947, Acc: 0.8759\n  Training Client 5...\n    Client 5 - Loss: 0.2932, Acc: 0.8736\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 4 Summary:\n    Avg Client Train Loss: 0.2946\n    Avg Client Train Acc:  0.8745\n    Global Val Loss:       0.2741\n    Global Val Acc:        0.8926\n    Global Val AUC:        0.9596\n    ✓ Checkpoint saved\n\n============================================================\nFEDERATED ROUND 5/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.2836, Acc: 0.8782\n  Training Client 2...\n    Client 2 - Loss: 0.2833, Acc: 0.8809\n  Training Client 3...\n    Client 3 - Loss: 0.2712, Acc: 0.8856\n  Training Client 4...\n    Client 4 - Loss: 0.2777, Acc: 0.8830\n  Training Client 5...\n    Client 5 - Loss: 0.2889, Acc: 0.8798\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 5 Summary:\n    Avg Client Train Loss: 0.2809\n    Avg Client Train Acc:  0.8815\n    Global Val Loss:       0.2759\n    Global Val Acc:        0.8832\n    Global Val AUC:        0.9548\n\n============================================================\nFEDERATED ROUND 6/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.2677, Acc: 0.8883\n  Training Client 2...\n    Client 2 - Loss: 0.2696, Acc: 0.8885\n  Training Client 3...\n    Client 3 - Loss: 0.2680, Acc: 0.8852\n  Training Client 4...\n    Client 4 - Loss: 0.2714, Acc: 0.8874\n  Training Client 5...\n    Client 5 - Loss: 0.2693, Acc: 0.8860\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 6 Summary:\n    Avg Client Train Loss: 0.2692\n    Avg Client Train Acc:  0.8871\n    Global Val Loss:       0.2457\n    Global Val Acc:        0.9015\n    Global Val AUC:        0.9654\n    ✓ Checkpoint saved\n\n============================================================\nFEDERATED ROUND 7/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.2564, Acc: 0.8936\n  Training Client 2...\n    Client 2 - Loss: 0.2672, Acc: 0.8870\n  Training Client 3...\n    Client 3 - Loss: 0.2609, Acc: 0.8916\n  Training Client 4...\n    Client 4 - Loss: 0.2591, Acc: 0.8909\n  Training Client 5...\n    Client 5 - Loss: 0.2567, Acc: 0.8923\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 7 Summary:\n    Avg Client Train Loss: 0.2601\n    Avg Client Train Acc:  0.8911\n    Global Val Loss:       0.2523\n    Global Val Acc:        0.8960\n    Global Val AUC:        0.9619\n\n============================================================\nFEDERATED ROUND 8/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.2514, Acc: 0.8966\n  Training Client 2...\n    Client 2 - Loss: 0.2504, Acc: 0.8939\n  Training Client 3...\n    Client 3 - Loss: 0.2454, Acc: 0.8972\n  Training Client 4...\n    Client 4 - Loss: 0.2573, Acc: 0.8940\n  Training Client 5...\n    Client 5 - Loss: 0.2497, Acc: 0.8932\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 8 Summary:\n    Avg Client Train Loss: 0.2508\n    Avg Client Train Acc:  0.8950\n    Global Val Loss:       0.2311\n    Global Val Acc:        0.9082\n    Global Val AUC:        0.9699\n    ✓ Checkpoint saved\n\n============================================================\nFEDERATED ROUND 9/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.2460, Acc: 0.8965\n  Training Client 2...\n    Client 2 - Loss: 0.2461, Acc: 0.8985\n  Training Client 3...\n    Client 3 - Loss: 0.2389, Acc: 0.9027\n  Training Client 4...\n    Client 4 - Loss: 0.2435, Acc: 0.8993\n  Training Client 5...\n    Client 5 - Loss: 0.2451, Acc: 0.8985\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 9 Summary:\n    Avg Client Train Loss: 0.2439\n    Avg Client Train Acc:  0.8991\n    Global Val Loss:       0.2245\n    Global Val Acc:        0.9129\n    Global Val AUC:        0.9719\n\n============================================================\nFEDERATED ROUND 10/10\n============================================================\n  Training Client 1...\n    Client 1 - Loss: 0.2275, Acc: 0.9074\n  Training Client 2...\n    Client 2 - Loss: 0.2316, Acc: 0.9039\n  Training Client 3...\n    Client 3 - Loss: 0.2365, Acc: 0.9014\n  Training Client 4...\n    Client 4 - Loss: 0.2311, Acc: 0.9061\n  Training Client 5...\n    Client 5 - Loss: 0.2438, Acc: 0.8987\n\n  Aggregating weights (weighted by dataset size)...\n  Evaluating global model on validation set...\n\n  Round 10 Summary:\n    Avg Client Train Loss: 0.2341\n    Avg Client Train Acc:  0.9035\n    Global Val Loss:       0.2309\n    Global Val Acc:        0.9047\n    Global Val AUC:        0.9679\n    ✓ Checkpoint saved\n\n============================================================\nFINAL EVALUATION ON TEST SET\n============================================================\n\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 201ms/step - accuracy: 0.9541 - auc: 0.9891 - loss: 0.1352\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\nFinal Test Results:\n  Test Loss:     0.2287\n  Test Accuracy: 90.32%\n  Test AUC:      0.9686\n\n✓ Training Complete!\n✓ Model saved as 'federated_final_model.h5'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}